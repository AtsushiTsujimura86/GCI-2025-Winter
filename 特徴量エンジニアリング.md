# 🧰 特徴量エンジニアリングまとめ（実務レベル）

機械学習モデルの性能を上げるための「具体的なテクニック＋コード例」をまとめたチートシート。

---

# 1. 数値変数の変換

## 標準化（Standardization）
**目的：** 平均0・分散1に揃えて距離・勾配計算を安定化  
- 身長180と視力1.0では、モデルに与える影響が大きく偏る
- 距離系・勾配系モデルでは必須、決定木では不要
- 標準化は $Z=\frac{X-\mu}{\sigma}$ の式で表される。Xはデータ、 $\mu$ は平均値、 $\sigma$ は標準偏差




**使う場面：** ロジスティック回帰 / SVM / ニューラルネットなど

```
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

## 正規化（Normalization）
**目的：** 0〜1 にスケールする

```
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_norm = scaler.fit_transform(X)
```

## 非線形変換（歪み補正）
分布の偏りを軽減し、モデルの学習を安定化。numpyの`.log1p()`を使う。

### 対数変換
```
df["Fare_log"] = np.log1p(df["Fare"])
```

### 平方根変換
```
df["Age_sqrt"] = np.sqrt(df["Age"])
```

### Box-Cox変換
```
from scipy.stats import boxcox
df["Fare_boxcox"], lambda_ = boxcox(df["Fare"] + 1)
```

## 交差項（Interaction）
特徴量同士の相互作用を追加する。

```
df["Age*Fare"] = df["Age"] * df["Fare"]
```

---

# 2. カテゴリカル変数の変換

## ラベルエンコーディング
簡単にカテゴリを数値化できる。決定木系モデル（LightGBM, XGBoost）向け。
例) (male, female) -> (1.0)

```
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["Sex_le"] = le.fit_transform(df["Sex"])
```

## カウントエンコーディング
カテゴリの“頻度”を特徴にする。

```
df["Cabin_count"] = df["Cabin"].map(df["Cabin"].value_counts())
```

## One-Hot Encoding（OHE）
線形モデル・NN で最強。カテゴリの中の種類の数だけ新たに列を追加して、True, Falseで表す（dtype=intで01にする）。
class = (A,B,C)の３種類の場合、新たにA, B, Cという列を追加する。

```
df = pd.get_dummies(df, columns=["Embarked"] , dtype=int)
```

## ターゲットエンコーディング
**目的：** カテゴリと目的変数の関係を直接エンコード  
**注意：** 過学習しやすいので必ず KFold 併用

```
from category_encoders import TargetEncoder
te = TargetEncoder(cols=["Cabin"])
df["Cabin_te"] = te.fit_transform(df["Cabin"], df["Survived"])
```

---

# 3. 時系列データの扱い

## 比率特徴量
```
df["sales_ratio"] = df["sales"] / df["sales"].shift(1)
```

## 移動平均
```
df["temp_ma3"] = df["temp"].rolling(window=3).mean()
```

## 周期性（sin/cos）
24時間や12ヶ月の“円環性”を表現。

```
df["hour_sin"] = np.sin(2*np.pi*df["hour"]/24)
df["hour_cos"] = np.cos(2*np.pi*df["hour"]/24)
```

---

# 4. 特徴選択

## RFE（再帰的特徴量削除）

```
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
selector = RFE(model, n_features_to_select=10)
selector.fit(X, y)

selected_features = X.columns[selector.support_]
```

## 重要度ベース（Treeモデル）
```
model = RandomForestClassifier()
model.fit(X, y)
importances = model.feature_importances_
```

## 相関ベース（多重共線性の除去）
```
corr = X.corr()
# |相関| > 0.9 の特徴量ペアを削除候補に
```

---

# 5. 欠損値処理（前処理の最重要部分）

## 平均・中央値補完
```
df["Age"].fillna(df["Age"].median(), inplace=True)
```

## カテゴリの欠損
```
df["Embarked"].fillna("Missing", inplace=True)
```

## KNN補完
```
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=3)
df_filled = imputer.fit_transform(df)
```

---

# 6. スケーリングの使い分け

| 手法 | 適したモデル | 例 |
|------|--------------|----|
| 標準化 | ロジ回 / SVM / NN | LogisticRegression, SVC |
| 正規化 | 距離系 | KNN |
| 不要 | 決定木系 | LightGBM, XGBoost |

---

# 7. Titanic でよく作る特徴量集

## 家族人数
```
df["FamilySize"] = df["SibSp"] + df["Parch"] + 1
```

## 一人フラグ
```
df["IsAlone"] = (df["FamilySize"] == 1).astype(int)
```

## 敬称（Title）
```
df["Title"] = df["Name"].str.extract("([A-Za-z]+)\.")
```

## チケット文字数
```
df["Ticket_len"] = df["Ticket"].apply(lambda x: len(str(x)))
```

---

# ✔ 最後に

特徴量エンジニアリングは  
「どんな特徴を追加するとモデルの理解が深まるか」を考える作業。  

- 分布を整える  
- カテゴリを正しく数値化する  
- 意味のある特徴を追加する  
- 過学習しないよう特徴を選ぶ  

この流れで進めるとモデル性能が安定する。

