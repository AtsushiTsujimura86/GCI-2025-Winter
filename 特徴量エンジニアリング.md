# 🧰 特徴量エンジニアリングまとめ（実務レベル）

機械学習モデルの性能を上げるための「具体的なテクニック＋コード例」をまとめたチートシート。

---

# 1. 数値変数の変換

## 標準化（Standardization）
**目的：** 平均0・分散1に揃えて距離・勾配計算を安定化  
- 身長180と視力1.0では、モデルに与える影響が大きく偏る
- 距離系・勾配系モデルでは必須、決定木では不要
- 標準化は $Z=\frac{X-\mu}{\sigma}$ の式で表される。Xはデータ、 $\mu$ は平均値、 $\sigma$ は標準偏差
- 
**使う場面：** ロジスティック回帰 / SVM / ニューラルネットなど

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

## 正規化（Normalization）
**目的：** 0〜1 にスケールする。距離ベースのモデルで特に有効。

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_norm = scaler.fit_transform(X)
```


##  スケーリングの使い分け

| 手法 | 適したモデル | 例 |
|------|--------------|----|
| 標準化 | ロジ回 / SVM / NN | LogisticRegression, SVC |
| 正規化 | 距離系 | KNN |
| 不要 | 決定木系 | LightGBM, XGBoost |

---

## 非線形変換（歪み補正）
分布の偏りを軽減し、モデルの学習を安定化。※ `log1p` = log(x+1)。0でも安全。
### 対数変換
```python
df["Fare_log"] = np.log1p(df["Fare"])
```

### 平方根変換
```python
df["Age_sqrt"] = np.sqrt(df["Age"])
```

### Box-Cox変換
```python
from scipy.stats import boxcox
df["Fare_boxcox"], lambda_ = boxcox(df["Fare"] + 1)
```

## 交差項（Interaction）
特徴量同士の相互作用を追加する。線形モデルでよく使う。
```python
df["Age*Fare"] = df["Age"] * df["Fare"]
```

---

# 2. カテゴリカル変数の変換

## ラベルエンコーディング
簡単にカテゴリを数値化できる。決定木系モデル（LightGBM, XGBoost）向け。
例) (male, female) -> (1.0)

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["Sex_le"] = le.fit_transform(df["Sex"])
```

## カウントエンコーディング
カテゴリの“頻度”を特徴にする。

```python
df["Cabin_count"] = df["Cabin"].map(df["Cabin"].value_counts())
```

## One-Hot Encoding（OHE）
線形モデル・NN で最強。カテゴリの中の種類の数だけ新たに列を追加して、True, Falseで表す（dtype=intで01にする）。
class = (A,B,C)の３種類の場合、新たにA, B, Cという列を追加する。

```python
df = pd.get_dummies(df, columns=["Embarked"] , dtype=int)
```

## ターゲットエンコーディング
**目的：** カテゴリと目的変数の関係を直接エンコード  
**注意：** 過学習しやすいので必ず KFold 併用

```python
from category_encoders import TargetEncoder
te = TargetEncoder(cols=["Cabin"])
df["Cabin_te"] = te.fit_transform(df["Cabin"], df["Survived"])
```

---

# 3. 時系列データの扱い

## ◆ 比率特徴量（変化・トレンドをとらえる最強手法）
`shift(n)` は「n行上（過去）にずらす」関数。ここではsales_ratio列に、その行のsales / 一つ上の行のsalseを入れている。
つまり、ひとつ前のsalsesからの変化量を表している。
```python
df["sales_ratio"] = df["sales"] / df["sales"].shift(1)
```
**Tips**
- 生データより比率の方が “変化” がわかる  
- 絶対値の大小より “どれくらい変わったか” が強い特徴に  
- 0割り回避のため、分母には +1 を加えることも多い  
```python
df["sales_ratio"] = df["sales"] / (df["sales"].shift(1) + 1)
```

---

## ◆ 移動平均（ノイズ削減＋トレンド抽出）
`rolling(window=n)` は「直近 n 個の値を対象に計算する」。`rolling(window=3).mean()`は直近3個の値の平均値
```python
df["temp_ma3"] = df["temp"].rolling(window=3).mean()
```

**Tips**
- window=7（日次データなら週単位）が鉄板  
- ノイズをならすためモデルの安定性が上がる  
- 予測タスクでは「未来の値を使わない」ように注意（リーク）

---

## ◆ 周期性（sin/cos） → MLで時刻情報を扱う最強技
```python
df["hour_sin"] = np.sin(2*np.pi*df["hour"]/24)
df["hour_cos"] = np.cos(2*np.pi*df["hour"]/24)
```

**Tips**
- “23時 → 0時” が近いことを表現できる（One-Hotでは無理）  
- 月（12周期）、曜日（7周期）にも応用可能  
- これを知らないと LGBM 以外でスコアが伸びない

---

# 4. 特徴選択（Feature Selection）

## ◆ RFE（再帰的特徴量削除）
モデルにおける特徴量の重要度が低い順に、再帰的に特徴量を減らしていく。
1. 全部の特徴で学習
2. 重要度の低いものを削除
3. また学習
4. 目標の数になるまで繰り返す
```python
selector = RFE(LogisticRegression(), n_features_to_select=10)
selector.fit(X, y)
```

**Tips**
- ロジ回/SVMなど「線形モデル」と相性抜群  
- 多すぎる特徴（>300）を扱うときに効果的  
- “重要度低い順に除去” でノイズ削減  
- 木系モデルではあまり使わない（理由：木は自分で特徴選択する）

---

## ◆ 重要度ベース（Treeモデル最強の特徴選択）
特徴量の重要度(feature_importances)を参考に特徴量を削る手法。  
【概念】feature_importances_とは？

決定木系モデル（RandomForest / XGBoost / LightGBM）が
**「どの特徴で分岐すると精度が上がったか」**を計算して
“どの特徴が重要だったか” を数値で教えてくれるもの。

数値が大きいほど **「この特徴が生死の判断に効いた」**という意味。
```python
rf = RandomForestClassifier().fit(X, y)
importances = rf.feature_importances_
```

**Tips**
- LGBM、XGBoost、RF ではこの方式が最強  
- importance が 0 に近い特徴は削除候補  
- ただし：
  - 重要度は安定しないことが多い  
  - 重要度は相関の影響を受ける  
  - 一度だけの importance は信用しすぎない  
- → KFold で importance を安定化すると良い

---

## ◆ 相関ベース（多重共線性除去）  
相関の高い変数の片方を削除する。
※多重共線性：特徴量同士が強く相関（似た値）になってしまっている状態 ⇒ モデルが混乱する
例）“Fare” と “Fare_log”、“身長” と “cm換算した身長”、“SibSp＋Parch＋1＝FamilySize” と元の SibSp/Parchなど
```python
corr = X.corr()
```

**Tips**
- |相関| > 0.90 のペアはどちらか削除してOK  
- 重回帰やロジ回では多重共線性が致命傷  
- LightGBM では致命傷にはならないが “無駄な特徴” になる  
- 特に：
  - “元の値” と “log変換値”  
  - “日付” と “曜日”  
  - “カテゴリのOHE列どうし”  
  は高相関になりがち

---

# ✨ 特徴選択 全体のTipsまとめ

- 木系モデル（LightGBM・XGBoost）は  
  **「特徴選択しなくても強い」** → でも削ると速度UP
- 線形モデル（ロジ回・SVM）は  
  **「特徴選択しないと精度が悪化しやすい」**
- 相関の高い特徴を減らすと  
  **モデルの安定性（バラツキ）が小さくなる**
- 特徴量は"多けりゃいい"ではなく  
  **“意味のある入力だけ” が強いモデルを作る**


---

# 5. 欠損値処理（前処理の最重要部分）

## 平均・中央値補完
```python
df["Age"].fillna(df["Age"].median(), inplace=True)
```
※`inplace=True`は、元のDataFrameを書き換える（＝コピーを作らずにその場で上書きする）設定  
つまり df = df.fillna(...) と書かなくても、df.fillna(..., inplace=True) だけで df が直接変更される という意味

## カテゴリの欠損
欠損値を"Missing"という値で補完。欠損していることに何か意味がある可能性があるため。
```python
df["Embarked"].fillna("Missing", inplace=True)
```

## KNN補完
“欠損してるサンプルと似てるデータ” を探してその平均などで埋める方法。単純な平均補完より自然な値になりやすい。  
【注意】計算が重い、数値に強くカテゴリには弱い、過学習気味になる場合もあるので、使い所は慎重に
```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=3)
df_filled = imputer.fit_transform(df)
```



---

# 7. Titanic でよく作る特徴量集

## 家族人数
```python
df["FamilySize"] = df["SibSp"] + df["Parch"] + 1
```

## 一人フラグ
```python
df["IsAlone"] = (df["FamilySize"] == 1).astype(int)
```

## 敬称（Title）
```python
df["Title"] = df["Name"].str.extract("([A-Za-z]+)\.")
```

## チケット文字数
```python
df["Ticket_len"] = df["Ticket"].apply(lambda x: len(str(x)))
```

### .apply(lambda x: func(x))の意味
Series（列）の値を **1つずつ取り出して、ラムダ関数で処理する方法**。  

#### `df["col"].apply(lambda x: func(x))`の動作
1. 列の値を **1行ずつ取り出す**  
2. 取り出した値を **x に代入**  
3. `func(x)` を実行して結果を返す  
4. それを新しい列として返す

---

#### 🧠 処理のイメージ
```python
df["Age"] = [10, 20, 30]

df["Age"].apply(lambda x: x + 5)
```
実際の動作：
- x=10 → 15  
- x=20 → 25  
- x=30 → 35  
→ `[15, 25, 35]` が返る。

---

#### 自作関数と組み合わせた例
```python
def categorize(age):
    if age < 18:
        return "child"
    else:
        return "adult"

df["AgeGroup"] = df["Age"].apply(lambda x: categorize(x))
```

---

#### よく使うパターン

1. 値を変換
```python
df["Fare2"] = df["Fare"].apply(lambda x: x ** 2)
```

2. 文字列処理
```python
df["FirstName"] = df["Name"].apply(lambda x: x.split(",")[0])
```

3. 正規表現と組み合わせ
```python
import re
df["Num"] = df["Ticket"].apply(lambda x: re.sub(r"\D", "", x))
```
※re.sub()は正規表現でマッチしたところを置き換える。この場合、r"\D"(数字以外)を ""(無、つまり削除)で置き換える。rは""で囲んだ中をrawデータとして扱うようにするプレフィックス。
これがないと、"\\D"のように、エスケープのために毎回\\を書かないといけない。pythonでは大量に\\を使うため、r""はよく使う。


4.  条件分岐
```python
df["IsChild"] = df["Age"].apply(lambda x: 1 if x < 15 else 0)
```

---




